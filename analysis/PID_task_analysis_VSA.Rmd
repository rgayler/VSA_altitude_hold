---
title: "VSA Implementation of PID Task Analysis"
author: "Ross Gayler"
date: "2021-0"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

suppressPackageStartupMessages(
  {
    library(fs)
    library(here)
    library(vroom)
    library(dplyr)
    library(stringr)
    library(ggplot2)
    library(tibble)
    library(tidyr)
    library(glmnet)
    library(glmnetUtils)
    library(lattice)
  }
)

source(here::here("R", "functions.R"))
```

This notebook attempts to make a VSA implementation of the statistical
model of the PID input/output function developed in the [PID Task
Analysis notebook](PID_task_analysis.html).

The scalar inputs are represented using the spline encoding developed in
the [Linear Interpolation Spline notebook](encoder_spline_html).

The VSA representations of the inputs are informed by the encodings
developed in the [feature engineering
section](PID_task_analysis.html#feature-engineering).

The knots of the splines are chosen to allow curvature in the response
surface where it is needed.

The input/output function will be learned by regression from the
multi-target data set.

$e$ and $e_{orth}$ will be used as the input variables and I will ignore
the VSA calculation of those quantities in this notebook (except for the
final section where I unsuccessfully attempt to use $k_{tgt}$, $z$, and
$dz$ as the input variables).

We will build up the model incrementally to check that things are
working as expected.

# Read data

This notebook is based on the data generated by the simulation runs with
multiple target altitudes per run.

Read the data from the simulations and mathematically reconstruct the
values of the nodes not included in the input files.

Only a small subset of the nodes will be used in this notebook.

```{r}
# function to clip value to a range
clip <- function(
  x, # numeric
  x_min, # numeric[1] - minimum output value
  x_max  # numeric[1] - maximum output value
) # value # numeric - x constrained to the range [x_min, x_max]
  {
  x %>% pmax(x_min) %>% pmin(x_max)
}

# function to extract a numeric parameter value from the file name
get_param_num <- function(
  file, # character - vector of file names
  regexp # character[1] - regular expression for "param=value"
         # use a capture group to get the value part
) # value # numeric - vector of parameter values
{
  file %>% str_match(regexp) %>% 
      subset(select = 2) %>% as.numeric()
}

# function to extract a sequence of numeric parameter values from the file name
get_param_num_seq <- function(
  file, # character - vector of file names
  regexp # character[1] - regular expression for "param=value"
         # use a capture group to get the value part
) # value # character - character representation of a sequence, e.g. "(1,2,3)"
{
  file %>% str_match(regexp) %>% 
      subset(select = 2) %>% 
    str_replace_all(c("^" = "(", "_" = ",", "$" = ")")) # reformat as sequence
}

# function to extract a logical parameter value from the file name
get_param_log <- function(
  file, # character - vector of file names
  regexp # character[1] - regular expression for "param=value"
  # use a capture group to get the value part
  # value *must* be T or F
) # value # logical - vector of logical parameter values
{
  file %>% str_match(regexp) %>% 
    subset(select = 2) %>% as.character() %>% "=="("T")
}

# read the data
d_wide <- fs::dir_ls(path = here::here("data", "multiple_target"), regexp = "/targets=.*\\.csv$") %>% # get file paths
  vroom::vroom(id = "file") %>% # read files
  dplyr::rename(k_tgt = target) %>% # rename for consistency with single target data
  dplyr::mutate( # add extra columns
    file = file %>% fs::path_ext_remove() %>% fs::path_file(), # get file name
    # get parameters
    targets  = file %>% get_param_num_seq("targets=([._0-9]+)_start="), # hacky
    k_start  = file %>% get_param_num("start=([.0-9]+)"), 
    sim_id   = paste(targets, k_start), # short ID for each simulation
    k_p      = file %>% get_param_num("kp=([.0-9]+)"),
    k_i      = file %>% get_param_num("Ki=([.0-9]+)"),
    # k_tgt    = file %>% get_param_num("k_tgt=([.0-9]+)"), # no longer needed
    k_windup = file %>% get_param_num("k_windup=([.0-9]+)"),
    uclip    = FALSE, # constant across all files
    dz0      = TRUE, # constant across all files
    # Deal with the fact that the interpretation of the imported u value
    # depends on the uclip parameter
    u_import = u, # keep a copy of the imported value to one side
    u = dplyr::if_else(uclip, # make u the "correct" value
                       u_import, 
                       clip(u_import, 0, 1)
                       ),
    # reconstruct the missing nodes
    i1 = k_tgt - z,
    i2 = i1 - dz,
    i3 = e * k_p,
    i9 = lag(ei, n = 1, default = 0), # initialised to zero
    i4 = e + i9,
    i5 = i4 %>% clip(-k_windup, k_windup),
    i6 = ei * k_i,
    i7 = i3 + i6,
    i8 = i7 %>% clip(0, 1),
    # add a convenience variable for these analyses
    e_orth = (i1 + dz)/2 # orthogonal input axis to e
  ) %>% 
  # add time variable per file
  dplyr::group_by(file) %>% 
  dplyr::mutate(t = 1:n()) %>% 
  dplyr::ungroup()

dplyr::glimpse(d_wide)
```

# Marginal $e$ term

Model the marginal effect of the $e$ term.

Check the distribution of $e$.

```{r}
summary(d_wide$e)

d_wide %>% 
  ggplot2::ggplot() +
  geom_vline(xintercept = c(-0.1, 0, 0.1), colour = "red") +
  geom_histogram(aes(x = e), bins = 100) +
  scale_y_log10()
```

-   $e$ ranges from -6 to 6 with.
-   $e$ values are heavily concentrated in the interval $[-0.1, 0.1]$
    (the terminal phases of the trajectories).

Construct a spline specification with knots set at
$\{-6, -2, -0.1, 0, 0.1, 2, 6\}$. The linear spline encoding
interpolates linearly between knots, so the knots at $\{-0.1, 0, 0.1\}$
allow for extreme curvature of the motor command surface in that region.
The knot at 2 is to allow for the slope of motor command surface
observed on the range $0 \le e \le 2$.

```{r}
vsa_dim <- 1e4L # default VSA dimensionality

ss_e <- vsa_mk_scalar_encoder_spline_spec(vsa_dim = vsa_dim, knots = c(-6, -0.1, 0, 0.1, 2, 6))
```

In using regression to decode VSA representations I will have to create
a data matrix with one row per observation (the encoding of the value
$e$). One column will correspond to the dependent variable - the
*target* value $u$. The remaining columns correspond to the independent
variables (predictors) - the VSA vector representation of the encoded
scalar value $e$. There will be one column for each element of the VSA
vector representation.

The number of columns will exceed the number of rows, so simple
unregularised regression will fail. I will initially attempt to use
regularised regression (elasticnet, implemented by `glmnet` in R). Prior
experimentation showed that the optimal $\alpha = 0$ (ridge regression).

I use a random sample of observations from the simulation data. This is
done because my laptop struggles with larger problems.

```{r}
# number of observations in subset of data for modelling
n_samp <- 1000

# select the modelling subset
d_subset <- d_wide %>% 
  dplyr::slice_sample(n = n_samp) %>% 
  dplyr::select(u, e, e_orth, k_tgt, z, dz) # k_tgt, z, dz only used in the last section
```

Create a function to encode scalar values as VSA vectors and convert the
data to a matrix suitable for later regression modelling.

```{r}
# function to to take a set of numeric scalar values,
# encode them as VSA vectors using the given spline specification,
# and package the results as a data frame for regression modelling
mk_df <- function(
  x, # numeric - vector of scalar values to be VSA encoded
  ss # data frame - spline specification for scalar encoding
) # value - data frame - one row per scalar, one column for the scalar and each VSA element
{
  tibble::tibble(x_num = x) %>% # put scalars as column of data frame
    dplyr::rowwise() %>% 
    dplyr::mutate( 
      x_vsa = x_num %>% vsa_encode_scalar_spline(ss) %>% # encode each scalar
        tibble(vsa_i = 1:length(.), vsa_val = .) %>% # name all the vector elements
        tidyr::pivot_wider(names_from = vsa_i, names_prefix = "vsa_", values_from = vsa_val) %>% # transpose vector to columns
        list() # package the value for a list column
    ) %>% 
    dplyr::ungroup() %>%
    tidyr::unnest(x_vsa) %>%  # convert the nested df to columns
    dplyr::select(-x_num) # drop the scalar value that was encoded
}
```

Create the training data from the subset of observations from the
simulation data. This only contains the outcome $u$ and the VSA encoding
of the predictor $e$.

```{r}
# create training data
# contains: u, encode(e)
d_train <- dplyr::bind_cols(
  dplyr::select(d_subset, u), 
  mk_df(dplyr::pull(d_subset, e), ss_e)
)

# take a quick look at the data
d_train
```

Create the test data corresponding to a regular grid of $e$ values. This
only contains the predictor $e$ and the VSA encoding of $e$.

```{r}
# create testing data
# contains: e, encode(e)
e_test <- seq(from = -6, to = 6, by = 0.05) # grid of e values
d_test <- dplyr::bind_cols(
  tibble::tibble(e = e_test), 
  mk_df(e_test, ss_e)
)

# take a quick look at the data
d_test
```

Fit a linear (i.e. Gaussian family) regression model, predicting the
scalar value $u$ (motor command) from the VSA vector elements.

Use $\alpha = 0$ (ridge regression).

`cva.glmnet()` in the code below runs cross-validation fits across a
grid of $\lambda$ values so that we can choose the best regularisation
scheme. The $\lambda$ parameter is the weighting of the elastic-net
penalty.

```{r}
# fit a set of models at a grid of alpha and lambda parameter values
fit_1 <- glmnetUtils::cva.glmnet(u ~ ., data = d_train, family = "gaussian", alpha = 0)
```

Look at the impact of the $\lambda$ parameter on goodness of fit for the
$\alpha = 0$ curve.

```{r}
# extract the alpha = 0 model
fit_1_alpha_0 <- fit_1$modlist[[1]]

# look at the lambda curve
plot(fit_1_alpha_0)

# get a summary of the alpha = 0 model
fit_1_alpha_0
```

-   The left dotted vertical line corresponds to minimum error. The
    right dotted vertical line corresponds to the largest value of
    lambda such that the error is within one standard-error of the
    minimum - the so-called "one-standard-error" rule.
-   The numbers along the top margin show the number of nonzero
    coefficients in the regression models corresponding to the lambda
    values. All the plausible models have a number of nonzero
    coefficients equal to roughly 97% of the dimensionality of the VSA
    vectors.

Prior experimentation found that the minimising $\lambda$ is preferred
over the model selected by the one-standard-error rule. The more heavily
penalised model tends not to predict the extreme values of the outcome
variable (too much shrinkage).

Look at how well the learned function reconstructs the previously
observed empirical relation between $u$ and $e$.

```{r}
# add the predicted values to the test data frame
d_test <- d_test %>% 
  dplyr::mutate(u_hat = predict(fit_1, newdata = ., alpha = 0, s = "lambda.min")) %>%
  dplyr::relocate(u_hat)

# plot of predicted u vs e
d_test %>% 
  ggplot() + 
  geom_vline(xintercept = c(-6, -0.1, 0, 0.1, 2, 6), colour = "skyblue3") + # knots
  geom_hline(yintercept = c(0, 1), colour = "darkgrey") + # limits of u
  geom_smooth(data = d_wide, aes(x = e, y = u), method = "gam", formula = y ~ s(x, bs = "ad", k = 150)) +
  geom_point(data = d_wide, aes(x = e, y = u), colour = "red", alpha = 0.2) +
  geom_point(aes(x = e, y = u_hat))
```

The vertical blue lines indicate the locations of the knots on $e$.

The horizontal grey lines indicate the limits of the motor command $u$.

The red points are all the observations from the simulation data set.

The smooth blue curve with the grey confidence band is an adaptive
smooth summarising the trend of the red points.

The black points are the $u$ values learned from the VSA vector
representation.

-   That's a pretty good fit.

-   The reconstruction is slightly underfit (the reconstructed values
    are slightly closer to 0.5 than the actual values). This is most
    obvious in the outermost intervals ($e \lt -0.1$ and \$e \gt 2).

    -   This is probably due to the regularisation (the regression
        coefficients are shrunk towards zero) but is worth checking for
        other explanations.

# Marginal $e_{orth}$ term

Model the marginal effect of the $e_{orth}$ term.

Check the distribution of $e_{orth}$.

```{r}
summary(d_wide$e_orth)

d_wide %>% 
  ggplot2::ggplot() +
  geom_vline(xintercept = c(-0.1, 0, 0.1), colour = "red") +
  geom_histogram(aes(x = e_orth), bins = 100) +
  scale_y_log10()
```

-   $e_{orth}$ ranges from -5 to 5.
-   $e_{orth}$ values are concentrated in the interval $[-2, 2]$ with a
    notch at zero.

Construct a spline specification with knots set at ${-5, 5}$. The
earlier analyses showed that $u$ is a linear function of $e_{orth}$ when
$e \approx 0$ (otherwise $u$ is unrelated to $e_{orth}$). Having only
two knots will force the spline interpolation to be a linear function of
$e_{orth}$

```{r}
ss_eo <- vsa_mk_scalar_encoder_spline_spec(vsa_dim = vsa_dim, knots = c(-5, 5))
```

Restrict this analysis to the observations with $e \approx 0$.

```{r}
d_subset_eo <- d_subset %>% 
  dplyr:: filter(abs(e) < 0.1)

dim(d_subset_eo)
```

-   This subset retains \~90% of the original observations. (Only \~10%
    of the observations come from the initial phases of the
    trajectories.)

Create the training data from the relevant subset of observations from
the simulation data. This only contains the outcome $u$ and the VSA
encoding of the predictor $e_{orth}$.

```{r}
# create training data
# contains: u, encode(e_orth)
d_train <- dplyr::bind_cols(
  dplyr::select(d_subset_eo, u), 
  mk_df(dplyr::pull(d_subset_eo, e_orth), ss_eo)
  )

# take a quick look at the data
d_train
```

Create the test data corresponding to a regular grid of $e_{orth}$
values. This only contains the predictor $e_{orth}$ and the VSA encoding
of $e_{orth}$.

```{r}
# create testing data
# contains: e_orth, encode(e_orth)
eo_test <- seq(from = -5, to = 5, by = 0.05) # grid of e_orth values
d_test <- dplyr::bind_cols(
  tibble::tibble(e_orth = eo_test), 
  mk_df(eo_test, ss_eo)
  )

# take a quick look at the data
d_test
```

Fit a linear (i.e. Gaussian family) regression model, predicting the
scalar value $u$ (motor command) from the VSA vector elements.

Use $\alpha = 0$ (ridge regression).

```{r}
# fit a set of models at a grid of alpha and lambda parameter values
fit_2 <- glmnetUtils::cva.glmnet(u ~ ., data = d_train, family = "gaussian", alpha = 0)
```

Look at the impact of the $\lambda$ parameter on goodness of fit for the
$\alpha = 0$ curve.

```{r}
# extract the alpha = 0 model
fit_2_alpha_0 <- fit_2$modlist[[1]]

# look at the lambda curve
plot(fit_2_alpha_0)

# get a summary of the alpha = 0 model
fit_2_alpha_0
```

-   The left dotted vertical line corresponds to minimum error. The
    right dotted vertical line corresponds to the largest value of
    lambda such that the error is within one standard-error of the
    minimum - the so-called "one-standard-error" rule.
-   The numbers along the top margin show the number of nonzero
    coefficients in the regression models corresponding to the lambda
    values. All the plausible models have a number of nonzero
    coefficients equal to roughly 50% of the dimensionality of the VSA
    vectors.

Prior experimentation found that the minimising $\lambda$ is preferred
over the model selected by the one-standard-error rule. The more heavily
penalised model tends not to predict the extreme values of the outcome
variable (too much shrinkage).

Look at how well the learned function reconstructs the previously
observed empirical relation between $u$ and $e_{orth}$.

```{r}
# add the predicted values to the test data frame
d_test <- d_test %>% 
  dplyr::mutate(u_hat = predict(fit_2, newdata = ., alpha = 0, s = "lambda.min")) %>%
  dplyr::relocate(u_hat)

# plot of predicted u vs e
d_test %>% 
  ggplot() + 
  geom_vline(xintercept = c(-5, 5), colour = "skyblue3") + # knots
  geom_hline(yintercept = c(0, 1), colour = "darkgrey") + # limits of u
  # geom_smooth(data = dplyr::filter(d_wide, abs(e) < 0.1), aes(x = e_orth, y = u), method = "gam", formula = y ~ s(x, bs = "ad")) +
  geom_point(data = dplyr::filter(d_wide, abs(e) < 0.1), aes(x = e_orth, y = u), colour = "red", alpha = 0.2) +
  geom_point(aes(x = e_orth, y = u_hat))
```

The vertical blue lines indicate the locations of the knots on
$e_{orth}$.

The horizontal grey lines indicate the limits of the motor command $u$.

The red points are all the observations from the simulation data set
with $-0.1 \lt e \lt 0.1$.

The black points are the $u$ values learned from the VSA vector
representation.

-   The fit is linear, as expected from only having two knots.

-   The reconstruction is slightly underfit (the reconstructed slope is
    slightly less than the trend of the actual data.

    -   This is probably due to the regularisation (the regression
        coefficients are shrunk towards zero) but is worth checking for
        other explanations.

-   The dispersion of the red points around the black trend line are due
    to overshoot and oscillation in the simulated PID controller.

# Joint $e$ and $e_{orth}$ term

Model the joint effect of $e$ and $e_{orth}$.

We are trying to learn a function $e \times e_{orth} \to u$, so it's
appropriate to use $encode(e) \otimes encode(e_{orth})$ as the VSA
predictor. This equivalent to modelling $u$ from the Cartesian product
of the knots for $e$ and $e_{orth}$.

Create a function to encode scalar values of $e$ and $e_{orth}$ as VSA
vectors, calculate the VSA product of their representations, and convert
the data to a matrix suitable for later regression modelling.

This is particularly appropriate because the curvature of the empirical
$u$ surface is axis parallel to $e$ and $e_{orth}$.

```{r}
# function to to take two sets of numeric scalar values,
# encode them as VSA vectors using the given spline specifications,
# calculate the VSA product of the two VSA representations
# and package the results as a data frame for regression modelling
mk_df_j2 <- function(
  x_e, # numeric - vectors of scalar values to be VSA encoded
  x_eo,
  ss_e, # data frame - spline specifications for the scalar encodings
  ss_eo
) # value - data frame - one row per scalar, one column for the scalar and each VSA element
{
  tibble::tibble(x_e = x_e, x_eo = x_eo) %>% # put scalars as column of data frame
    dplyr::rowwise() %>% 
    dplyr::mutate( 
      x_vsa = vsa_multiply(
        vsa_encode_scalar_spline(x_e, ss_e),
        vsa_encode_scalar_spline(x_eo, ss_eo)
      ) %>% 
        tibble(vsa_i = 1:length(.), vsa_val = .) %>% # name all the vector elements
        tidyr::pivot_wider(names_from = vsa_i, names_prefix = "vsa_", values_from = vsa_val) %>% # transpose vector to columns
        list() # package the value for a list column
    ) %>% 
    dplyr::ungroup() %>%
    tidyr::unnest(x_vsa) %>%  # convert the nested df to columns
    dplyr::select(-x_e, -x_eo) # drop the scalar value that was encoded
}
```

Create the training data from the subset of observations from the
simulation data. This only contains the outcome $u$ and the VSA encoding
of the predictor $encode(e) \otimes encode(e_{orth})$\$.

```{r}
# create training data
# contains: u, encode(e) * encode(e_orth)
d_train <- dplyr::bind_cols(
  dplyr::select(d_subset, u), 
  mk_df_j2(dplyr::pull(d_subset, e), dplyr::pull(d_subset, e_orth), ss_e, ss_eo)
)

# take a quick look at the data
d_train
```

Create the test data corresponding to a regular grid of $e$ and
$e_{orth}$ values. This only contains the predictors $e$ $e_{orth}$, and
the VSA encoding of $encode(e) \otimes encode(e_{orth})$.

```{r}
# create testing data
# contains: e, e_orth, encode(e) * encode(e_orth)
e_test <- c(seq(from = -6, to = -1, by = 1), seq(from = -0.2, to = 0.2, by = 0.05), seq(from = 1, to = 6, by = 1)) # grid of e values
eo_test <- seq(from = -5, to = 5, by = 1) # grid of e values
d_grid <- tidyr::expand_grid(e = e_test, e_orth = eo_test)

d_test <- dplyr::bind_cols(
  d_grid,
  mk_df_j2(dplyr::pull(d_grid, e), dplyr::pull(d_grid, e_orth), ss_e, ss_eo)
)

# take a quick look at the data
d_test
```

Fit a linear (i.e. Gaussian family) regression model, predicting the
scalar value $u$ (motor command) from the VSA vector elements.

Use $\alpha = 0$ (ridge regression).

```{r}
# fit a set of models at a grid of alpha and lambda parameter values
fit_3 <- glmnetUtils::cva.glmnet(u ~ ., data = d_train, family = "gaussian", alpha = 0)
```

Look at the impact of the $\lambda$ parameter on goodness of fit for the
$\alpha = 0$ curve.

```{r}
# extract the alpha = 0 model
fit_3_alpha_0 <- fit_3$modlist[[1]]

# look at the lambda curve
plot(fit_3_alpha_0)

# get a summary of the alpha = 0 model
fit_3_alpha_0
```

-   The left dotted vertical line corresponds to minimum error. The
    right dotted vertical line corresponds to the largest value of
    lambda such that the error is within one standard-error of the
    minimum - the so-called "one-standard-error" rule.
-   The numbers along the top margin show the number of nonzero
    coefficients in the regression models corresponding to the lambda
    values. All the plausible models have a number of nonzero
    coefficients equal to roughly 98% of the dimensionality of the VSA
    vectors.

Prior experimentation found that the minimising $\lambda$ is preferred
over the model selected by the one-standard-error rule. The more heavily
penalised model tends not to predict the extreme values of the outcome
variable (too much shrinkage).

Look at how well the learned function reconstructs the previously
observed empirical relation from $e \times e_{orth}$ to $u$.

```{r}
# add the predicted values to the test data frame
d_test <- d_test %>% 
  dplyr::mutate(u_hat = predict(fit_3, newdata = ., alpha = 0, s = "lambda.min")) %>%
  dplyr::relocate(u_hat)

# plot of predicted u vs e * e_orth
d_test %>% 
  ggplot() + 
  geom_point(aes(x = e, y = e_orth, size = u_hat, colour = u_hat)) +
  scale_color_viridis_c()

lattice::wireframe(u_hat ~ e * e_orth, data = d_test, drape = TRUE)

d_test %>% 
  ggplot() + 
  geom_point(aes(x = e_orth, y = u_hat)) +
  facet_wrap(facets = vars(e))
```

-   That looks like a pretty good reconstruction of the empirical
    relationship from $e$ and $e_{orth}$ to $u$.

    -   The overall dependence of $u$ on the sign of $e$ is obvious.
    -   The linear relationship from $e_{orth}$ to $u$ when
        $e \approx 0$ has been captured.

So, that works reasonably well.

# Joint $k_{tgt}$, $z$, and $dz$ term

The predictors in the earlier sections, $e$ and $e_{orth}$, are
deterministic functions of $k_{tgt}$, $z$, and $dz$.

In principle, we should be able to learn the function
$k_{tgt} \times z \times dz \to u$. However, the curvature in the $u$
surface would no longer be axis-parallel, and it would not be possible
to hand-craft the knots on the variables to reflect the demands of the
task.

Consequently, I think it's likely a naive approach to doing this won't
work.

Model the joint effect of $k_{tgt}$, $z$, and $dz$.

Check the distributions of input variables.

```{r}
summary(d_wide$k_tgt)

d_wide %>% 
  ggplot2::ggplot() +
  geom_histogram(aes(x = k_tgt), bins = 100)
```

```{r}
summary(d_wide$z)

d_wide %>% 
  ggplot2::ggplot() +
  geom_histogram(aes(x = z), bins = 100) +
  scale_y_log10()
```

```{r}
summary(d_wide$dz)

d_wide %>% 
  ggplot2::ggplot() +
  geom_histogram(aes(x = dz), bins = 100) +
  scale_y_log10()
```

We can't choose the knots to meet the expected task demands, so just
encode each predictor with a uniformly spaced grid of knots.

Choose the number of knots to make the spacing relatively close (to try
to capture regions of sharp curvature), On the other hand, the linear
spline encoding scheme only interpolates between adjacent knots, so it
can't generalise over greater distances. Using a large number of knots
makes the learning process more like a nearest neighbour lookup process,
so sparsity of data relative to the grid of knots will become a problem.

Create the spline specifications for each of the predictor variables.

```{r}
ss_tgt <- vsa_mk_scalar_encoder_spline_spec(vsa_dim = vsa_dim, knots = seq(from = 1, to = 8, by = 0.5))
ss_z <- vsa_mk_scalar_encoder_spline_spec(vsa_dim = vsa_dim, knots = seq(from = 1, to = 8, by = 0.5))
ss_dz <- vsa_mk_scalar_encoder_spline_spec(vsa_dim = vsa_dim, knots = seq(from = -5, to = 5, by = 0.5))
```

We are trying to learn a function $k_{tgt} \times z \times dz \to u$, so
it's appropriate to use
$encode(k_{tgt}) \otimes encode(z) \otimes encode(dz)$ as the VSA
predictor. This is equivalent to modelling $u$ from the Cartesian
product of the knots for $k_{tgt}$, $z$, and $dz$.

Create a function to encode scalar values of $k_{tgt}$, $z$, and $dz$ as
VSA vectors, calculate the VSA product of their representations, and
convert the data to a matrix suitable for later regression modelling.

```{r}
# function to to take 3 sets of numeric scalar values,
# encode them as VSA vectors using the given spline specifications,
# calculate the VSA product of the VSA representations
# and package the results as a data frame for regression modelling
mk_df_j3 <- function(
  x_tgt, # numeric - vectors of scalar values to be VSA encoded
  x_z,
  x_dz,
  ss_tgt,# data frame - spline specifications for scalar encoding
  ss_z,
  ss_dz 
) # value - data frame - one row per 3 scalars, one column for each scalar and each VSA element
{
  tibble::tibble(x_tgt = x_tgt, x_z = x_z, x_dz = x_dz) %>% # put scalars as columns of data frame
    dplyr::rowwise() %>% 
    dplyr::mutate( 
      x_vsa = vsa_multiply(
        vsa_encode_scalar_spline(x_tgt, ss_tgt),
        vsa_encode_scalar_spline(x_z, ss_z),
        vsa_encode_scalar_spline(x_dz, ss_dz)
      ) %>% 
        tibble(vsa_i = 1:length(.), vsa_val = .) %>% # name all the vector elements
        tidyr::pivot_wider(names_from = vsa_i, names_prefix = "vsa_", values_from = vsa_val) %>% # transpose vector to columns
        list() # package the value for a list column
    ) %>% 
    dplyr::ungroup() %>%
    tidyr::unnest(x_vsa) %>%  # convert the nested df to columns
    dplyr::select(-x_tgt, -x_z, -x_dz) # drop the scalar values that were encoded
}
```

Create the training data from the subset of observations from the
simulation data. This only contains the outcome $u$ and the VSA encoding
of the predictor $encode(k_{tgt}) \otimes encode(z) \otimes encode(dz)$.

```{r}
# create training data
# contains: u, encode(e)
d_train <- dplyr::bind_cols(
  dplyr::select(d_subset, u), 
  mk_df_j3(dplyr::pull(d_subset, k_tgt), dplyr::pull(d_subset, z), dplyr::pull(d_subset, dz), ss_tgt, ss_z, ss_dz)
)

# take a quick look at the data
d_train
```

Create the test data corresponding to a regular grid of $k_{tgt}$, $z$,
and $dz$ values. Calculate $e$ and $e_{orth}$ from the input values. Add
the VSA vector encoding of
$encode(k_{tgt}) \otimes encode(z) \otimes encode(dz)$.

Filter the observations to only have values of $e$ and $e_{orth}$ that
are in the range observed in the simulation data.

```{r}
# create testing data
# contains: k_tgt, z, dz, e, e_orth, encode(k_tgt) * encode(z) * encode(dz)
tgt_test <- 1:8 # grid of k_tgt values
z_test <- seq(from = 1, to = 8, by = 0.5) # grid of z values
dz_test <- seq(from = -5, to = 5, by = 0.5) # grid of dz values
d_grid <- tidyr::expand_grid(k_tgt = tgt_test, z = z_test, dz = dz_test) %>% 
  dplyr::mutate(
    i1 = k_tgt - z,
    e = i1 - dz,
    e_orth = (i1 + dz)/2
  ) %>% 
  dplyr::filter(abs(e)<= 6 & abs(e_orth) <= 5) # remove extreme values of e and e_orth

d_test <- dplyr::bind_cols(
  d_grid,
  mk_df_j3(dplyr::pull(d_grid, k_tgt), dplyr::pull(d_grid, z), dplyr::pull(d_grid, dz), ss_tgt, ss_z, ss_dz)
)

# take a quick look at the data
d_test
```

Fit a linear (i.e. Gaussian family) regression model, predicting the
scalar value $u$ (motor command) from the VSA vector elements.

Use $\alpha = 0$ (ridge regression).

```{r}
# fit a set of models at a grid of alpha and lambda parameter values
fit_4 <- glmnetUtils::cva.glmnet(u ~ ., data = d_train, family = "gaussian", alpha = 0)
```

Look at the impact of the $\lambda$ parameter on goodness of fit for the
$\alpha = 0$ curve.

```{r}
# extract the alpha = 0 model
fit_4_alpha_0 <- fit_4$modlist[[1]]

# look at the lambda curve
plot(fit_4_alpha_0)

# get a summary of the alpha = 0 model
fit_4_alpha_0
```

-   The left dotted vertical line corresponds to minimum error. The
    right dotted vertical line corresponds to the largest value of
    lambda such that the error is within one standard-error of the
    minimum - the so-called "one-standard-error" rule.
-   The numbers along the top margin show the number of nonzero
    coefficients in the regression models corresponding to the lambda
    values. All the plausible models have a number of nonzero
    coefficients equal to the dimensionality of the VSA vectors.

Prior experimentation found that the minimising $\lambda$ is preferred
over the model selected by the one-standard-error rule. The more heavily
penalised model tends not to predict the extreme values of the outcome
variable (too much shrinkage).

Look at how well the learned function reconstructs the previously
observed empirical relation from $e \times e_{orth}$ to $u$.

```{r}
# add the predicted values to the test data frame
d_test <- d_test %>% 
  dplyr::mutate(u_hat = predict(fit_4, newdata = ., alpha = 0, s = "lambda.min")) %>%
  dplyr::relocate(u_hat)
```

Check the distributions of $e$ and $e_{orth}$ values (remember - these
were calculated from the grid of input variables).

```{r}
summary(d_test$e)

d_test %>% 
  ggplot2::ggplot() +
  geom_histogram(aes(x = e), bins = 100)

summary(d_test$e_orth)

d_test %>% 
  ggplot2::ggplot() +
  geom_histogram(aes(x = e_orth), bins = 100)

d_test %>% 
  ggplot2::ggplot() +
  geom_vline(xintercept = 0, colour = "red") +
  geom_jitter(aes(x = e, y = e_orth))
```

-   Reasonable coverage of the $e \times e_{orth}$ plane.

Check the distribution of predicted $u$ values.

```{r}
summary(d_test$u_hat)

d_test %>% 
  ggplot2::ggplot() +
  geom_histogram(aes(x = u_hat), bins = 100)
```

-   Too many central values and not enough extreme values. It looks like
    the model is underfit.

Look at how well the learned function reconstructs the previously
observed empirical relation between $u$ and $e$.

```{r}
# plot of predicted u vs e * e_orth
d_test %>% 
  ggplot() + 
  geom_jitter(aes(x = e, y = e_orth, colour = u_hat)) +
  scale_color_viridis_c()

d_test %>% 
  ggplot() + 
  geom_jitter(aes(x = e, y = e_orth, colour = u_hat)) +
  scale_color_viridis_c() +
  facet_wrap(facets = vars(k_tgt))

d_test %>% 
  ggplot() + 
  geom_point(aes(x = e, y = u_hat)) +
  geom_smooth(aes(x = e, y = u_hat)) +
  facet_wrap(facets = vars(k_tgt))

d_test %>% 
  dplyr::filter(abs(e) < 0.1) %>% 
  ggplot() + 
  geom_point(aes(x = e_orth, y = u_hat)) +
  geom_smooth(aes(x = e_orth, y = u_hat), method = "lm") +
  facet_wrap(facets = vars(k_tgt))
```

-   That's pretty awful.
