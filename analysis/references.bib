
@inproceedings{gaylerDistributedBasisAnalogical2009,
  title = {A Distributed Basis for Analogical Mapping},
  booktitle = {New {{Frontiers}} in {{Analogy Research}}, {{Proceedings}} of the {{Second International Conference}} on {{Analogy}}, {{ANALOGY}}-2009},
  author = {Gayler, Ross W. and Levy, Simon D.},
  editor = {Kokinov, Boicho and Holyoak, Keith J. and Gentner, Dedre},
  year = {2009},
  pages = {165--174},
  publisher = {{New Bulgarian University}},
  address = {{Sofia, Bulgaria}},
  abstract = {We are concerned with the practical fea- sibility of the neural basis of analogical map- ping. All existing connectionist models of ana- logical mapping rely to some degree on local- ist representation (each concept or relation is represented by a dedicated unit/neuron). These localist solutions are implausible because they need too many units for human-level compe- tence or require the dynamic re-wiring of net- works on a sub-second time-scale.  Analogical mapping can be formalised as finding an approximate isomorphism between graphs representing the source and target con- ceptual structures. Connectionist models of analogical mapping implement continuous heuristic processes for finding graph isomor- phisms. We present a novel connectionist mechanism for finding graph isomorphisms that relies on distributed, high-dimensional representations of structure and mappings. Consequently, it does not suffer from the prob- lems of the number of units scaling combinato- rially with the number of concepts or requiring dynamic network re-wiring.},
  copyright = {All rights reserved},
  keywords = {★},
  file = {/home/ross/Zotero/storage/VTA53S6V/Gayler, Levy - 2009 - A distributed basis for analogical mapping(2).pdf}
}

@misc{gaylerMultiplicativeBindingRepresentation1998,
  type = {Conference Poster},
  title = {Multiplicative {{Binding}}, {{Representation Operators}} \& {{Analogy}}},
  author = {Gayler, Ross W.},
  year = {1998},
  address = {{New Bulgarian University, Sofia, Bulgaria}},
  collaborator = {Holyoak, Keith and Gentner, Dedre and Kokinov, Boicho},
  copyright = {All rights reserved},
  keywords = {★,cogsci},
  file = {/home/ross/Zotero/storage/DQI6R7EY/Gayler - 1998 - Multiplicative Binding, Representation Operators &.pdf}
}

@misc{gaylerVSAVectorSymbolic2013,
  type = {Seminar},
  title = {{{VSA}}: {{Vector Symbolic Architectures}} for {{Cognitive Computing}} in {{Neural Networks}}},
  shorttitle = {{{VSA}} for {{Cognitive Computing}} in {{Neural Networks}}},
  author = {Gayler, Ross W.},
  year = {2013},
  month = jun,
  address = {{Redwood Center for Theoretical Neuroscience, UC Berkeley, Berkeley, CA, USA}},
  abstract = {This talk is about computing with discrete compositional data structures in analog computers. A core issue for both computer science and cognitive neuroscience is the degree of match between a class of computer designs and a class of computations. In cognitive science, it is manifested in the apparent mismatch between the neural network hardware of the brain (essentially, a massively parallel analog computer) and the computational requirements of higher cognition (statistical constraint processing with compositional discrete data structures to implement facilities such as language and analogical reasoning). Historically, analog computers have been considered ill-suited for implementing computational processes on discrete compositional data structures.    Neural networks can be construed as analog computers -- a class of computer design with a long history, but now relatively unknown. Historically, analog computation had advantages over digital computation in speed and parallelism. Computational problems were cast as dynamical systems and modelled by differential equations, which was relatively straightforward for models of physical problems such as flight dynamics. However, it was far less clear how to translate computations on discrete compositional data structures such as trees and graphs into dynamical systems. This is especially true for problems where the data structures evolve over time, implying the need to rewire the analog computer on the fly. This is particularly relevant to cognitive science because new concepts and relations can be created on the fly, and under the standard conception of neural networks this implies that neurons and connections would be created impossibly rapidly.    In this talk I describe Vector Symbolic Architectures, a family of mathematical techniques for analog computation in hyperdimensional vector spaces that map naturally onto neural network implementations. VSAs naturally support computation on discrete compositional data structures and a form of virtualisation that breaks the nexus between the items to be represented and the hardware that supports the representation. This means that computations on evolving data structures do not require physical rewiring of the implementing hardware. I illustrate this approach with a VSA system that finds isomorphisms between graphs and where different problems to be solved are represented by different initial states of the fixed hardware rather than by rewiring the hardware. Graph isomorphism is at the heart of the standard model of analogical reasoning and motivates this example, although that aspect is not explored in this talk.},
  copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
  language = {English},
  annotation = {slides: https://sites.google.com/site/rgayler/home/2013-06-14\%20Redwood\%20V1.pdf},
  file = {/home/ross/Zotero/storage/94RTBXJT/Gayler - 2013 - VSA Vector Symbolic Architectures (slides).pdf}
}

@article{kleykoVectorSymbolicArchitectures2021,
  title = {Vector {{Symbolic Architectures}} as a {{Computing Framework}} for {{Nanoscale Hardware}}},
  author = {Kleyko, Denis and Davies, Mike and Frady, E. Paxon and Kanerva, Pentti and Kent, Spencer J. and Olshausen, Bruno A. and Osipov, Evgeny and Rabaey, Jan M. and Rachkovskij, Dmitri A. and Rahimi, Abbas and Sommer, Friedrich T.},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.05268 [cs]},
  eprint = {2106.05268},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This article reviews recent progress in the development of the computing framework Vector Symbolic Architectures (also known as Hyperdimensional Computing). This framework is well suited for implementation in stochastic, nanoscale hardware and it naturally expresses the types of cognitive operations required for Artificial Intelligence (AI). We demonstrate in this article that the ring-like algebraic structure of Vector Symbolic Architectures offers simple but powerful operations on high-dimensional vectors that can support all data structures and manipulations relevant in modern computing. In addition, we illustrate the distinguishing feature of Vector Symbolic Architectures, "computing in superposition," which sets it apart from conventional computing. This latter property opens the door to efficient solutions to the difficult combinatorial search problems inherent in AI applications. Vector Symbolic Architectures are Turing complete, as we show, and we see them acting as a framework for computing with distributed representations in myriad AI settings. This paper serves as a reference for computer architects by illustrating techniques and philosophy of VSAs for distributed computing and relevance to emerging computing hardware, such as neuromorphic computing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Hardware Architecture},
  file = {/home/ross/Zotero/storage/3MLUPGN9/Kleyko et al. - 2021 - Vector Symbolic Architectures as a Computing Frame.pdf;/home/ross/Zotero/storage/9ZPWIE2T/2106.html}
}

@book{langtangenScalingDifferentialEquations2016,
  title = {Scaling of {{Differential Equations}}},
  author = {Langtangen, Hans Petter and Pedersen, Geir K.},
  year = {2016},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-32726-6},
  isbn = {978-3-319-32725-9 978-3-319-32726-6},
  language = {en},
  file = {/home/ross/Zotero/storage/QTNDG5BG/Langtangen and Pedersen - 2016 - Scaling of Differential Equations.pdf}
}

@article{schlegelComparisonVectorSymbolic2020,
  title = {A Comparison of {{Vector Symbolic Architectures}}},
  author = {Schlegel, Kenny and Neubert, Peer and Protzel, Peter},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.11797 [cs]},
  eprint = {2001.11797},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Vector Symbolic Architectures (VSAs) combine a high-dimensional vector space with a set of carefully designed operators in order to perform symbolic computations with large numerical vectors. Major goals are the exploitation of their representational power and ability to deal with fuzziness and ambiguity. Over the past years, VSAs have been applied to a broad range of tasks and several VSA implementations have been proposed. The available implementations differ in the underlying vector space (e.g., binary vectors or complex-valued vectors) and the particular implementations of the required VSA operators - with important ramifications for the properties of these architectures. For example, not every VSA is equally well suited to address each task, including complete incompatibility. In this paper, we give an overview of eight available VSA implementations and discuss their commonalities and differences in the underlying vector space, bundling, and binding/unbinding operations. We create a taxonomy of available binding/unbinding operations and show an important ramification for non self-inverse binding operation using an example from analogical reasoning. A main contribution is the experimental comparison of the available implementations regarding (1) the capacity of bundles, (2) the approximation quality of non-exact unbinding operations, and (3) the influence of combined binding and bundling operations on the query answering performance. We expect this systematization and comparison to be relevant for development and evaluation of new VSAs, but most importantly, to support the selection of an appropriate VSA for a particular task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/ross/Zotero/storage/NAQKAUUB/Schlegel et al. - 2020 - A comparison of Vector Symbolic Architectures.pdf}
}


